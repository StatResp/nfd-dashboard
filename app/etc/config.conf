#####  Basic: General users should define the following parameters #####

#The name of the input files in the input folder should be defined here. If you do not have the data, you can download it from here: https://vanderbilt365.sharepoint.com/sites/ScopeLabDataSets/Shared%20Documents/General/statresp/Final_datasets
[filepaths]
#The name and location of the merged dataframe in the input folder.
merged_pickle = merged/TDOT_merged_4h_2017-04-01_to_2021-06-01_top20percent_segments_grouped_nona
#The name and location of the incident dataframe in the input folder. It is used by run_simulation module.
incident_pickle = raw/cleaned/for_cloud/incident_tdot
#The name and location of the roadways dataframe in the input folder. It includes the geometry of the roadway segments. Most likely it is either grouped_line or inrix
inrix_pickle = raw/cleaned/for_cloud/inrix/grouped/grouped_line


#The hyperparameters for prediction are defined in this section.
[prediction]
#the time range of interest for prediction in %Y-%m-%d %H:%M:%S format. The user can choose up to 5 days in future. If the user choose a date in future, the model tries to collect/generate traffic and weather information. Currently, it works for dates between 2017/May/01 to 2021/Apr/01, or now to 5 days ahead.
start_time_predict = 2020-02-01 00:00:00
end_time_predict = 2020-02-7 00:00:00
#If user is interested in the prediction of the segments located in a specific county or counties, they can be defined here. For example, county_list=["davidson",'hamilton']. Otherwise, user should use county_list=["All"]
county_list=["davidson"]
#If user is interested in the prediction of a specific segment or segments, they can define it here. For example, segment_list=[3012476,3028463,3016482,3027686]. Otherwise, user should use segment_list=[]
segment_list=[]
#If user is interested in the prediction of the segments located in a specific boundary, they can define it here. The format is a nested list of at lest 3 pairs of coordinates with longitude first and latitude second. For example polygon=[[-86.925,36.050],[-86.545,36.050],[-86.545,36.245],[-86.925,36.245]]. Otherwise, user should use polygon=[].
polygon=[]
#The run_prediction.py script can use any trained model for prediction. The trained models are generally stored in data/trained_models/models, and user can choose one of them. In general, the user should use the best model based on the performance metrics. For example, model_to_predict=RF+RUS1.0+KM2-2017-05-01-000000to2021-04-29-000000.sav
model_to_predict=RF+RUS1.0+KM2-2017-05-01-000000to2021-04-29-000000.sav


#The following parameters are required by the run_simulation module. The parameters with values in brackets (num_ambulances, alpha, average_service_time, ambulance_avg_speed, penalty) are evaluated similar to a grid search, where all combinations of parameter values are evaluated. For example, if  num_ambulances=[5, 10, 15] and alpha=[0,1] this produces 6 simulations.
[simulation]
#A simulation can be run after run_training.py or run_prediction.py scripts and use either of their outputs. You can choose which one is of your interest. You can choose between trained_models and prediction_example, which is the name of the folders that run_training.py or run_prediction.py save their outputs, respectively.
source= prediction_example
#This defines the number of ambulances (resources). It should a be list of integers. For example, num_ambulances=[5, 10, 15]
num_ambulances=[5]
#This defines the balance parameter in the modified p-median problem. It should a be list of values bigger than 0. For example, num_ambulances=[0, 0.5, 1]
alpha=[0]
#This defines average service time hyperparameter. It is the average time in hour that takes the responder to clean the scene and become available again.  It should a be list of values. For example, average_service_time=[0, 0.5]
average_service_time=[0.5]
#This defines the average speed of of the ambulances in kph. It is used to define how long it takes for the ambulances to be on the scene.  It should a be list of values. For exmaple, ambulance_avg_speed=[60, 100]
ambulance_avg_speed=[100]
#The simulation automatically reports the number of missed incidents. However, if you want to incorporate it in the travel distance using a penalty factor, you can define the penalty in meters for each missed incident. For exmaple, penalty=[0, 1000]
penalty=[0]
#The number of the cores in your machine for parallel processing; put zero for auto, and put 1 for no parallel processing.
num_cores=1


#If the model requires access from external resources, it may need some keys. They should be provided here or they can be defined an environment variable.
[keys]
weatherbit_key  = None



#=======================================================================================================================
#####  Advanced: Only Advanced users can change the following parameters #####
#The name of the input files in the input folder should be defined here. If you do not have the data, you can download it from here: https://vanderbilt365.sharepoint.com/sites/ScopeLabDataSets/Shared%20Documents/General/statresp/Final_datasets
[filepaths_advanced]
#The name and location of the graph (network created using networkx) of the inrix segments. It is mainly used for Hierarchical Clustering in run_dataprep.py module.
graph_pickle = raw/cleaned/local/inrix/graph/inrix_graph.gpickle

#The features (columns in the merged file) that are used by the machine learning module to predict the likelihood of incidents should be provided here by category. The features should exist in the merged file.
[features]
# Features regarding time
features_temporal =  window, is_weekend
# Features regarding incidents
features_incident = count_incidents_exact_last_week
# Features regarding weather
features_weather = vis_mean, wind_spd_mean, precip_mean, temp_mean
# Features regarding traffic
features_traffic = congestion_mean, reference_speed_mean
# Features regarding static properties of the roadway segments
features_static = lanes, miles, isf_length_min
# If any of the aforementioned features are categorical, they should be mentioned here.
categorical_features = window, is_weekend


#The configuration parameters regarding the ML part of the engine should be defined here. You can define multiple models and compare and contrast their performance. Please be careful that all of the parameters the should have the same number of elements - i.e. the number of models listed for the 'model_type' parameter should be the same as the number of files listed for 'hyperparam_config_files', the number of clustering methods listed for 'cluster_type', and the other configuration parameters. These are used by the run_training module.
[mlModels]
#The type of the model should be defined here. For now, LR, NN, RF, and ZIP are available.
model_type = LR, LR, RF, NN
#some models, such as RF may require a lot of hyperparameters. The user should define them in a file and feed the address to the appropriate model.
hyperparam_config_files = ["","","etc/rf_configs/rf.json",""]
#The user can also add a naive model. The naive model is basically a hot-spot model which uses the average of all time historical incidents. It can be used as the baseline model. This is the only item in [mlModels] that affects both the run_dataprep and run_training modules.
naive=True


#The clustering features should be defined here. The number of clustering methods and the number of ML methods should be equal.
[clusterParams]
#The clustering methods to use for each model should be defined here. For now the user can choose between NoC (no clustering), KM (K-means), and AH (Agglomerative Hierarchical)
cluster_type =  NoC, KM, KM, KM
#Number of the clusters should be defined here, and it should an integer. If the user chooses NoC for the clustering type, they should choose -1 for the number of the clusters.
cluster_number = -1, 3, 2, 2


#The type of resampling and the resampling ratio should be defined here. The number of resampling methods and ratios and the number of ML methods should be equal.
[resamplingParams]
#Type or method of resampling should be defined here. For now, user can choose between NR (No Resampling), ROS (Random Over Sampling), and RUS (Random Under Sampling)
resampling_type=  NR, ROS, RUS, RUS
#The ratio/rate of the majority and the minority class in the resampling strategy. If user choose NR for resampling method, they should choose 1 for the resampling ratio/rate. For ROS and RUS, the ratio should be in the range (0, 1]
resampling_rate=  1, 0.5, 1, 1


#Some hyperparameters regarding the prediction pipeline should be defined here. In general, they are in common between all of the modules. Therefore, if you chagne them, you may need to run all of the 4 modules in a chain again.
[metadata]
#The size of the time window in seconds should be defined. For example, if 4-hour time window is of interest, then 4*60*60=14400 is the proper value for window_size
window_size = 14400
#The time zone of the time column should be defined. If it is not local, the model converts that to the local zone. The other option for now is time_zone = UTC. It is highly recommended that user follow the local time.
time_zone = Local
#The name of the column representing the roadway segment IDs. In general, xdsegid and grouped_xdsegid for when the inrix data is not grouped and is grouped, respectively.
unit_name = grouped_xdsegid
#The name of the column that includes the total number of incidents in time window and segment should be defined here. The values of this column can be any positive integer. In general, it is used by the count models such as ZIP
pred_name_count = count_incidents
#The name of the column that includes the presence of incidents in time window and segment should be defined here. The values of this column can be be either zero or one. In general, it is used by the classification models such as LR, NN, RF, etc.
pred_name_tf    = incident_occurred
#The seed number for reproducibility should be defined here. Any integer value is acceptable.
seed_number= 0
#User can choose different type of training and test strategies. The available options are simple, ratio_random, random_pseudo, moving_window_month, moving_window_week. 1) simple: it use the time ranges provided here for creating training and test datasets. 2)ratio_random: it takes the union of time ranges provided here and randomly selects some rows from that. random_pseudo: it takes the union of time ranges provided here and randomly (but hard-coded) selects 4 weeks from that for test dataset and keeps the rest for training dataset., moving_window_month: it is what used in our paper "Learning Incident Prediction Models Over Large Geographical Areas for Emergency Response." It uses the time range provided for training to create the training dataset. Then it uses the time range provided for testing to create the test dataset. If the time range for test is longer than 1 month, just the first month is selected for test and a pair of training and test dataset is created. Then, that one month rolls over to the training dataset and the next available month is used to create the next pair of training and test dataset. By doing so, multiple pairs are created. For example, if the time range for test is 3 months, 3 pairs are created.  moving_window_week: very similar to the previous one but it uses 1 week, rather than 1 month, as the unit for rolling over.
train_test_type =   moving_window_month
#If ratio_random is selected for train_test_type, the ratio should be defined here.
train_test_split = 0.8
#The ratio for validation dataset
train_verification_split = 0.8
#The range of the data the user wants to use from the merged dataframe for the training process. format: %Y-%m-%d %H:%M:%S
start_time_train = 2017-05-01 00:00:00
end_time_train = 2021-05-01 00:00:00
#The range of the data the user wants to use from the merged dataframe for the test process. format: %Y-%m-%d %H:%M:%S
start_time_test = 2021-04-01 00:00:00
end_time_test = 2021-04-29 00:00:00
#The model automatically creates some figures. To reduce the processing time, user can set figure_off False. 
figure_tag=True

#Some advanced parameters for prediction can be found here.
[prediction_advanced]
#The user can feed the prediction dataset into the model using a csv file. This can be useful for very small sensitivity analysis. For example, the predicted accident likelihood for a specific time and location is available through our model. However, the user is interested in know the change in the likelihood if the weather was not rainy that day but all of the other features were the same. The user can feed the model a csv file that all the features are the same as that specific day and location but just the feature regarding day is different.  If the user wants to opt for this option predict_data_from_csv=True otherwise predict_data_from_csv=False
predict_data_from_csv=False
#If user choose predict_data_from_csv=True, the name and location of the csv file in input folder should be defined here. Please pay attention that the format (the name of the columns) in the csv file should exactly follow the features that the pipeline requires.
prediction_dataset_csv_address = no

[simulation_advanced]
#you can choose the size of the grid in meters here. The grid is used for the potential locations of the responders in the run_simulation module.
grid_size = 5000




